---
title: "ABNAIA HPC Result Analysis"
output:
  rmarkdown::html_document:
    toc: true
    toc_depth: 2
    toc_float: true
    df_print: paged
  rmarkdown::html_vignette: default
# runtime: shiny
vignette: >
  %\VignetteIndexEntry{ABNAIA results analysis}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::knitr}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  cache.path="./"
)
```


```{r message=FALSE, warning=FALSE}
rm(list = ls())
library(parallel)
library(dplyr)
library(ggplot2)
library(tidyr)
library(reshape2)
library(svglite)
library(mcmcabn)
library(bnlearn)
# library(ExplorDataISGC)
# library(BNstructureLearning)
library(coda)
library(abn)
```

# Document Settings
```{r}
SAVEPLOTS <- T
PLOTPATH <- "./"
DATPATH <- "./"
PLOTWIDTH = 16
PLOTHEIGHT = 9
```

# Disclosure

BEWARE, THIS IS DRAFT CODE WITH PERSONAL NOTES OF MATTEO NOT INTENDED TO BE
REPRODUCIBLE IN ANY WAY!

# Experiment 6
## Load Data and take a first glimpse
```{r}
load(paste0(DATPATH, "exp6", "_final_100000.RData"))
```

FILENAME
```{r}
FILENAME <- paste0(FILENAME, "_100k")
FILENAME
```


List of all mcmcabn outputs. 
```{r}
length(mcmc.out.list)
```

Show first mcmcabn output.
```{r}
str(mcmc.out.list[1])
```

DAG from first mcmcabn output. [rows, columns, dags]. dags = number of returned DAGS specified in mcmcscheme.
```{r}
str(mcmc.out.list[[1]]$dags)
```

## Preprocess
### Thin and Burn-in

thinning: mcmc.scheme = c(number of returned DAGS, thinned steps, length of burn-in phase)

We thinning was set to keep the same amount of samples as in the non-parametric bootstrapping with tabu (10'000).

```{r}
# THINNING <- 2 # keep every second draw
# BURNIN.LEN <- 2500 # remove the first n draws
THINNING <- 7
BURNIN.LEN <- 25000 # remove the first n draws

mcmc.out.list.burn <- postBURNin(mcmc.out.list = mcmc.out.list, burnin.length = BURNIN.LEN)
mcmc.out.list.thin <- postTHINN(mcmc.out.list = mcmc.out.list, thinningsteps = THINNING)
mcmc.out.list.burn.thin <- postTHINN(mcmc.out.list = mcmc.out.list.burn, thinningsteps = THINNING)
str(mcmc.out.list.burn[1])
str(mcmc.out.list.thin[1])
str(mcmc.out.list.burn.thin[1])
```

### Reformat
```{r}
# thinned only
mc.out.thin.1 <- mcmc.out.list.thin[[1]]
mc.out.thin.2 <- mcmc.out.list.thin[[2]]
mc.out.thin.3 <- mcmc.out.list.thin[[3]]
mc.out.thin.4 <- mcmc.out.list.thin[[4]]

mc.out.thin.score.1 <- mcmc(mc.out.thin.1$scores)
mc.out.thin.score.2 <- mcmc(mc.out.thin.2$scores)
mc.out.thin.score.3 <- mcmc(mc.out.thin.3$scores)
mc.out.thin.score.4 <- mcmc(mc.out.thin.4$scores)

list.mc.out.thin.score <- mcmc.list(mc.out.thin.score.1, mc.out.thin.score.2, mc.out.thin.score.3, mc.out.thin.score.4)

# burned and thinned
mc.out.burn.thin.1 <- mcmc.out.list.burn.thin[[1]]
mc.out.burn.thin.2 <- mcmc.out.list.burn.thin[[2]]
mc.out.burn.thin.3 <- mcmc.out.list.burn.thin[[3]]
mc.out.burn.thin.4 <- mcmc.out.list.burn.thin[[4]]

mc.out.burn.thin.score.1 <- mcmc(mc.out.burn.thin.1$scores)
mc.out.burn.thin.score.2 <- mcmc(mc.out.burn.thin.2$scores)
mc.out.burn.thin.score.3 <- mcmc(mc.out.burn.thin.3$scores)
mc.out.burn.thin.score.4 <- mcmc(mc.out.burn.thin.4$scores)

# list.mc <- mcmc.list(mc.score.1, mc.score.2, mc.score.3, mc.score.4)
list.mc.out.burn.thin.score <- mcmc.list(mc.out.burn.thin.score.1, mc.out.burn.thin.score.2, mc.out.burn.thin.score.3, mc.out.burn.thin.score.4)

mc.out.burn.thin.dag.1 <- mc.out.burn.thin.1$dags
mc.out.burn.thin.dag.2 <- mc.out.burn.thin.2$dags
mc.out.burn.thin.dag.3 <- mc.out.burn.thin.3$dags
mc.out.burn.thin.dag.4 <- mc.out.burn.thin.4$dags

list.mc.out.burn.thin.dag <- abind::abind(mc.out.burn.thin.dag.1, mc.out.burn.thin.dag.2, mc.out.burn.thin.dag.3, mc.out.burn.thin.dag.4)
```

## Best fitting DAG
number of max parents per node
```{r}
max.par
```

total arcs
```{r}
# plot(dag.maxpar)
sum(dag.maxpar$dag)
summary(fabn.maxpar)
fabn.maxpar.bayes <- fitAbn(dag = dag.maxpar$dag,
              data.df = abndata,
              data.dists = dist, 
              method = "bayes",
              compute.fixed = T,
              create.graph = T)
```

### Prior distributions
```{r}
ncolsplot <- length(fabn.maxpar.bayes$marginals)
nrowsplot <- max.par+1
pltlayout <- layout(matrix(seq(1, ncolsplot*nrowsplot), ncol = ncolsplot, byrow = T))
# layout.show(pltlayout)
par(mar = c(rep(2.12, 4)))
for(i in 1:length(fabn.maxpar.bayes$marginals)){
  nom1<-names(fabn.maxpar.bayes$marginals)[i]
  cat("processing marginals for node:", nom1 ,"\n");
  cur.node.marg<-fabn.maxpar.bayes$marginals[i];
  cur.node.marg<-cur.node.marg[[1]];
  
  for(j in 1:length(cur.node.marg)){
    nom2<-names(cur.node.marg)[j]
    if (str_detect(nom2, "Intercept|precision")){
      cat("Skipping ", nom2)
      next
    } else{
      cat("processing parameter:",nom2,"\n")
    }
    cur.marg<-cur.node.marg[[j]];
    cur.ci <- fabn.maxpar.bayes[[21]][[i]][[j]]
    class(cur.ci) <- NULL # Remove class abnFit as it causes an errors downwards.
    cur.ci <- as.data.frame(cur.ci)
    plot(cur.marg,type="l",main=paste(nom1,":",nom2))
    abline(v = cur.ci$x) # TODO: Use color contour instead of lines.
  }
}
```


## MCMC Quality check
### Gelman
```{r}
gelman.diag(x = list.mc.out.thin.score,autoburnin = T) # if higher than 1.1 or 1.2, run chain longer to improve convergence
gelman.plot(list.mc.out.thin.score, autoburnin = T)

if (SAVEPLOTS){
  PLOTNAME <- "gelmanplot"
  dev.print(svg, filename = paste0(PLOTPATH, FILENAME, PLOTNAME, ".svg"), width = PLOTWIDTH, height = PLOTHEIGHT)
  dev.off()
} else {
  gelman.plot(list.mc.out.thin.score, autoburnin = T)
}
```

### Raftery
calculate no. of iterations and no. of burn-ins to satisfy specified conditions
```{r}
raftery.diag(unlist(list.mc.out.thin.score))
```

### Heidelberg and Welch Diagnostics
test H0: The Markov Chain is from a stationary distribution. If not passed, chain must run longer.

```{r}
for (chain in 1:length(list.mc.out.thin.score)){
  print("------------------------")
  print(paste("Chain no: ", chain))
  print(heidel.diag(list.mc.out.thin.score[[chain]]))
}

```

### trace plot
```{r}
mcmcabn::plot.mcmcabn(mcmc.out.list.burn.thin[[1]])
if (SAVEPLOTS){
  PLOTNAME <- "traceplot"
  dev.print(svg, filename = paste0(PLOTPATH, FILENAME, PLOTNAME, ".svg"), width = PLOTWIDTH, height = PLOTHEIGHT)
  
  dev.off()
  } else {
  mcmcabn::plot.mcmcabn(mcmc.out.list.burn.thin[[1]])
  }
```


```{r}
if (SAVEPLOTS){
  PLOTNAME <- "traceplot_maxscore"
  mcmcabn::plot.mcmcabn(mcmc.out.list.burn.thin[[1]], max.score = TRUE)
  dev.print(svg, filename = paste0(PLOTPATH, FILENAME, PLOTNAME, ".svg"), width = PLOTWIDTH, height = PLOTHEIGHT)
  dev.off()
  } else {
  mcmcabn::plot.mcmcabn(mcmc.out.list.burn.thin[[1]], max.score = TRUE)
  }
```


```{r}
if (SAVEPLOTS){
  PLOTNAME <- "traceplot_classic"
  plot(list.mc.out.burn.thin.score)
  dev.print(svg, filename = paste0(PLOTPATH, FILENAME, PLOTNAME, ".svg"), width = PLOTWIDTH, height = PLOTHEIGHT)
  dev.off()
  } else {
  plot(list.mc.out.burn.thin.score)
}
```

```{r}
## traceplot
fabn <-fitAbn(dag = dag.maxpar$dag,
              data.df = abndata,
              data.dists = dist, 
              method = METHOD) 

max.score <- -fabn$bic

dta <- data.frame(mc.out.thin.1[2:4],
                  mc.out.thin.2[2:4],
                  mc.out.thin.3[2:4],
                  mc.out.thin.4[2:4]) # Thinned but not burned

dta <- dta[,c(1,4,7,10)]
# dta <- dta[thin, ]
names(dta) <- c("Run1","Run2","Run3","Run4")
dta$X <- (1:length(dta$Run1))
dta <- reshape2::melt(dta, "X", value.name = "scores")

dta$cummax[1] <- dta$scores[1]
for (i in 2:length(dta$scores)) {
  if (dta$scores[i] > dta$cummax[i - 1]) {
    dta$cummax[i] <- dta$scores[i]
    } else {
      dta$cummax[i] <- dta$cummax[i - 1]
    }
}

# Create a text

original_plot <- ggplot(data = dta, aes_string(x = "X", y="scores", color = "variable")) +
  geom_line(alpha = 0.8,lwd=1.1) +
  geom_hline(yintercept = max.score,linetype = "dashed", color = "red", alpha = 1) +
  geom_text(aes(25, max.score, label = round(max.score,digits = 2), vjust = -0.5), color = "red", check_overlap = TRUE) +
  labs(x = "DAG index", y = "DAG scores", colour = "MCMC:") +
  ggpubr::theme_pubr()+
  ylim(min(dta$scores),max(dta$scores)) 
  # annotate("rect", xmin=0, xmax=BURNIN.LEN, ymin=min(dta$scores), ymax=max.score,alpha = .3) +
  # geom_text(aes(BURNIN.LEN/THINNING*0.5, min(dta$scores), label = "Burn-in phase", vjust = -0.5, hjust=0), color = "black", check_overlap = TRUE)
# print(original_plot)

# Plot
y_density <- cowplot::axis_canvas(original_plot, axis = "y", coord_flip = TRUE) +
  geom_density(data = dta, aes_string(x = "scores",fill = "factor(variable)"), alpha = 0.5) +
  coord_flip()

cummax_plt <- ggplot(data = dta, aes_string(x = "X", y="cummax", color = "variable")) +
  geom_line(alpha = 0.8,lwd=1.1, inherit.aes = T) +
  geom_point(aes_string(color = "variable"), inherit.aes = T)+
  geom_hline(yintercept = max.score,linetype = "dashed", color = "red", alpha = 1) +
  geom_text(aes(25, max.score, label = round(max.score,digits = 2), vjust = -0.5), color = "red", check_overlap = TRUE) +
  labs(x = "DAG index", y = "DAG scores", colour = "MCMC:") +
  ggpubr::theme_pubr()+
  ylim(min(dta$scores),max(dta$scores)) 
  # annotate("rect", xmin=0, xmax=BURNIN.LEN, ymin=min(dta$scores), ymax=max.score, alpha = .3) +
  # geom_text(aes(BURNIN.LEN/THINNING*0.5, min(dta$scores), label = "Burn-in phase", vjust = -0.5, hjust=0), color = "black", check_overlap = TRUE)
# cummax_plt

# create the combined plot
combined_plot <- cowplot::ggdraw(cowplot::insert_yaxis_grob(plot = original_plot, grob = y_density, position = "right"))
combined_cummax_plot <- cowplot::ggdraw(cowplot::insert_yaxis_grob(plot = original_plot, grob = cummax_plt, position = "right"))
# ggsave(paste0(FILENAMEbase, FILENAME, "traceplot_combined.png"),
#        plot = cowplot::ggdraw(cowplot::insert_yaxis_grob(plot = original_plot, grob = y_density, position = "right")),
#        width = 9,height = 7)
# dev.off()
# print(combined_plot)

if (SAVEPLOTS){
  PLOTNAME <- "traceplot_allruns"
  ggsave(plot = original_plot,
         filename = paste0(FILENAME, PLOTNAME, ".svg"),
         path = PLOTPATH,
         width = PLOTWIDTH, height = PLOTHEIGHT)

  PLOTNAME <- "traceplot_allruns_combined"
  ggsave(plot = combined_plot,
         filename = paste0(FILENAME, PLOTNAME, ".svg"),
         path = PLOTPATH,
         width = PLOTWIDTH, height = PLOTHEIGHT)  

  PLOTNAME <- "traceplot_allruns_cummax"
  ggsave(plot = cummax_plt,
         filename = paste0(FILENAME, PLOTNAME, ".svg"),
         path = PLOTPATH,
         width = PLOTWIDTH, height = PLOTHEIGHT)  
  } else {
  original_plot
  combined_plot 
  cummax_plt
}
```

### Consensus DAG
best dag trimmed for controlling overfitting
```{r}
# Best DAG not trimmed 
dag.mcmc.boot <- apply(list.mc.out.burn.thin.dag, 1:2, mean)
colnames(dag.mcmc.boot) <- rownames(dag.mcmc.boot) <- names(dist)

# Best DAG Trimmed on THRESHOLD
dag.mcmc.boot.th <- dag.mcmc.boot
dag.mcmc.boot.th[dag.mcmc.boot.th>THRESHOLD]<-1
dag.mcmc.boot.th[dag.mcmc.boot.th<=THRESHOLD]<-0

# Plot Best DAG trimmed
# svg(filename = paste0(FILENAMEbase, FILENAME, "consensus_dag.svg"))
cons.dag.plt <- plotAbn(dag = dag.mcmc.boot.th,data.dists = dist)
# dev.off()


if (SAVEPLOTS){
  PLOTNAME <- "consensus_dag"
  dev.print(svg, filename = paste0(PLOTPATH, FILENAME, PLOTNAME, ".svg"), width = PLOTWIDTH/2, height = PLOTHEIGHT/2)
  cons.dag.plt 
  dev.off()
  abn::toGraphviz(dag.mcmc.boot.th, 
                  data.dists = dist, 
                  data.df = abndata, 
                  outfile =paste0(PLOTPATH, FILENAME, PLOTNAME, ".dot"),
                  directed=TRUE)
  saveRDS(dag.mcmc.boot.th, file = paste0(PLOTPATH, FILENAME, PLOTNAME, ".rds"))

  } else {
  cons.dag.plt
  }
```


### significance threshold:
```{r}
dag.mcmc.boot.stren <- as.vector(round(dag.mcmc.boot, 3))
arc.stren.sign.threshold <-arc.stren.threshold(dag.mcmc.boot.stren)


# relative arc strength
plot(ecdf(dag.mcmc.boot.stren))
abline(v = arc.stren.sign.threshold, lty=2)
abline(v=0.5)

# # absolute arc strength
# plot(ecdf(apply(list.mc.out.burn.thin.dag, 1:2, sum)))

if (SAVEPLOTS){
  PLOTNAME <- "cdf_arcstrength"
  dev.print(svg, filename = paste0(PLOTPATH, FILENAME, PLOTNAME, ".svg"), width = PLOTWIDTH/2, height = PLOTHEIGHT/2)
  dev.off()
  } 
```

Difference in DAG between two arc strength thresholds:
```{r}
# relative frequency of an arc appearing in the MCMC sample
x <- dag.mcmc.boot

x[which(x<THRESHOLD & x>arc.stren.sign.threshold)]
x[which(x<THRESHOLD & x>arc.stren.sign.threshold-0.01)] <- 100 # Assign impossible value to highlight arc
x
```


```{r}
# Fit best DAG trimmed 
fabn.boot.th.mle <-fitAbn(dag = dag.mcmc.boot.th,
              data.df = abndata,
              data.dists = dist, 
              method = METHOD,
              compute.fixed = T,
              create.graph = T)

fabn.boot.th.mle$bic
infoDag(dag.mcmc.boot.th)

# plot with arc strength
plotdag <- dag.mcmc.boot
plotdag[plotdag>THRESHOLD]<-1
plotdag[plotdag<=THRESHOLD]<-0

fitvals <- fabn.boot.th.mle$coef

for (i in 1:length(fitvals)){
  names(fitvals[[i]]) <- colnames(fitvals[[i]])
}

edgestren <- round(dag.mcmc.boot, 2)
edgestren[edgestren<=THRESHOLD]<-0

# svg(filename = paste0(FILENAMEbase, FILENAME, "consensus_dag_edgestrength.svg"))
cons.dag.plt.edgestrength <- plotAbn(dag = plotdag,
        data.dists = dist,
        # fitted.values = fitvals,
        digits = 2,
        edge.strength = edgestren)
# dev.off()

if (SAVEPLOTS){
  PLOTNAME <- "consensus_dag_edgestrength"
  dev.print(svg, filename = paste0(PLOTPATH, FILENAME, PLOTNAME, ".svg"), width = PLOTWIDTH/2, height = PLOTHEIGHT/2)
  cons.dag.plt.edgestrength
  dev.off()
  } else {
  plot(cons.dag.plt.edgestrength)
}
```


## Inference
Fit the data to the Network.
Can be either Bayesian or Maximum Likelihood estimation. 
Both implementations produce very similar results as shown in (Kratzer et al 2018, Information-Theoretic Scoring Rules to Learn
Additive Bayesian Network Applied to Epidemiology)

```{r}
# Fit best trimmed DAG with Bayesian framework to compute marginals
fabn.boot.th.bayes <- fitAbn(dag = dag.mcmc.boot.th,
              data.df = abndata,
              data.dists = dist, 
              method = "bayes",
              compute.fixed = T,
              create.graph = T,
              max.iters = 1000)

# Fit best trimmed DAG with MLE framework to compute marginals
fabn.boot.th.mle <-fitAbn(dag = dag.mcmc.boot.th,
              data.df = abndata,
              data.dists = dist, 
              method = "mle",
              compute.fixed = T,
              create.graph = T)

out.bayes <- unlist(fabn.boot.th.bayes$modes)
out.mle <- unlist(fabn.boot.th.mle$coef)
```

```{r}
##numeric
df.bayes <- as.data.frame(out.bayes) %>%
  rownames_to_column()%>%
  filter(!str_detect(rowname, "precision") &
           !str_detect(rowname, "Intercept"))
df.mle <- as.data.frame(out.mle) %>%
  rownames_to_column()%>%
  filter(!str_detect(rowname, "precision") &
           !str_detect(rowname, "Intercept"))

df.bayes
df.mle
```


##### Plot posterior marginal distribution for each parameter seperately
For example, the probability that the Hypertension|Gender parameter fals within a given range with it's credible intervals.
```{r}
ci <- fabn.boot.th.bayes[[21]][[4]][[2]]
class(ci) <- NULL
ci <- as.data.frame(ci)

plot(fabn.boot.th.bayes[[20]][[4]][[2]])
abline(v = ci$x)
```

We do that for all of the parameters.
```{r fig.show="hold", out.width="50%"}
ncolsplot <- 4
nrowsplot <- 5
pltlayout <- layout(matrix(seq(1, ncolsplot*nrowsplot), ncol = ncolsplot, byrow = T))
# layout.show(pltlayout)
par(mar = c(rep(2.12, 4)))
for(i in 1:length(fabn.boot.th.bayes$marginals)){
  nom1<-names(fabn.boot.th.bayes$marginals)[i]
  cat("processing marginals for node:", nom1 ,"\n");
  cur.node.marg<-fabn.boot.th.bayes$marginals[i];
  cur.node.marg<-cur.node.marg[[1]];
  
  for(j in 1:length(cur.node.marg)){
    nom2<-names(cur.node.marg)[j]
    if (str_detect(nom2, "Intercept|precision")){
      cat("Skipping ", nom2)
      next
    } else{
      cat("processing parameter:",nom2,"\n")
    }
    cur.marg<-cur.node.marg[[j]];
    cur.ci <- fabn.boot.th.bayes[[21]][[i]][[j]]
    class(cur.ci) <- NULL # Remove class abnFit as it causes an errors downwards.
    cur.ci <- as.data.frame(cur.ci)
    plot(cur.marg,type="l",main=paste(nom1,":",nom2))
    abline(v = cur.ci$x) # TODO: Use color contour instead of lines.
  }
}

if (SAVEPLOTS){
  PLOTNAME <- "post_marg_dist"
  dev.print(svg, filename = paste0(PLOTPATH, FILENAME, PLOTNAME, ".svg"), width = PLOTWIDTH, height = PLOTHEIGHT)
  dev.off()
}
```

##### Area under the marginal densities
```{r}
marnew <- fabn.boot.th.bayes$marginals[[1]]
for(i in 2: length(fabn.boot.th.bayes$marginals)){
  marnew <- c(marnew, fabn.boot.th.bayes$marginals[[i]])
}
myarea<-rep(NA,length(marnew))
names(myarea)<-names(marnew)

for(i in 1:length(marnew)){
  tmp<-spline(marnew[[i]]);
  myarea[i]<-sum(diff(tmp$x)*tmp$y[-1]);
}
```


```{r}
myarea.df <- as.data.frame(myarea) %>% 
  rownames_to_column() %>%
  mutate(rowname = factor(rowname),
         color = factor(str_extract(rowname, "[^|]+")))  # match 1 or more character before |

aucplot <- ggplot(myarea.df)+
  geom_col(aes(y = myarea, x= rowname, fill = color)) +
  coord_flip()+
  xlab("Area Under Density")+
  ylab("") +
  guides(fill=FALSE) + # Removes legend of fill
  theme_classic()

if (SAVEPLOTS){
  ggsave(aucplot, file = paste0(PLOTPATH, FILENAME, "auc_plot.png"))
  } else {
    print(aucplot)
  }
```

```{r}
margs <- marnew
mymat<-matrix(rep(NA,length(margs)*3),ncol=3)
rownames(mymat)<-names(margs)
colnames(mymat)<-c("2.5%","50%","97.5%")

ignore.me<-union(grep("\\(Int",names(margs)),grep("prec",names(margs)))
comment<-rep("",length(margs));
for(i in 1:length(margs)){
  tmp<-margs[[i]];
  tmp2<-cumsum(tmp[,2])/sum(tmp[,2]);
  mymat[i,]<-c(tmp[which(tmp2>0.025)[1]-1,1],
               tmp[which(tmp2>0.5)[1],1],
               tmp[which(tmp2>0.975)[1],1]);
  myvec<-mymat[i,];
  
  if( !(i%in%ignore.me) &&  (myvec[1]<0 && myvec[3]>0)){comment[i]<-"not sig. at 5%";} 
  
  mymat[i,]<-as.numeric(formatC(mymat[i,],digits=3,format="f"));
}
cbind(mymat, comment)

```

Marginals posterior distribution of the parameter estimates
```{r}
for(i in 1:length(fabn.boot.th.bayes$marginals)){
  for (j in 1:length(fabn.boot.th.bayes$marginals[[i]])) {
   plot(fabn.boot.th.bayes$marginals[[i]][[j]],type="l",main=names(fabn.boot.th.bayes$marginals)[[i]]); 
  }
}

## now for a table of quantiles
margs<-fabn.boot.th.bayes$marginals;
# mymat<-matrix(rep(NA,length(margs)*3),ncol=3);
result.table <- matrix(ncol = 4)
childParent <- c()
# rownames(mymat)<-names(margs);
colnames(result.table)<-c("childrenParent", "2.5%","50%","97.5%");
ignore.me<-union(grep("\\(Int",names(margs)),grep("prec",names(margs)));## these are not effect parameters - background constants and precisions
comment<-rep("",length(margs));
for(i in 1:length(margs)){
  tmp<-margs[[i]]
  childParent <- names(tmp)
  
  tempmat <- matrix(c(childParent,
                    rep(NA, length(childParent)*3)),
                    nrow = length(childParent))
  for (j in 1:length(tmp)) {
    tmp2<-cumsum(tmp[[j]][,2])/sum(tmp[[j]][,2]);
    tempmat[j,2] <- tmp[[j]][which(tmp2>0.025)[1]-1,1] ## -1 is so use value on the left of the 2.5% 
    tempmat[j,3] <- tmp[[j]][which(tmp2>0.5)[1],1]
    tempmat[j,4] <- tmp[[j]][which(tmp2>0.975)[1],1]
    
    # mymat <- matrix(childParent, ci1, ci2, ci3)
    # if( !(i%in%ignore.me) &&  (myvec[1]<0 && myvec[3]>0)){
    #   comment[i]<-"not sig. at 5%";} 
    ## truncate for printing
    # mymat[i,]<-as.numeric(formatC(mymat[i,],digits=3,format="f"));
  }
  result.table <- rbind(result.table, tempmat)
}
result.table.bayes <- as.data.frame(result.table[which(!str_detect(result.table[,1], "Intercept|precision")),]) %>%
  mutate(across(.cols = 2:4, function(x){round(as.numeric(x),3)})) 

for(i in 1:nrow(result.table.bayes)){
  if (result.table.bayes[i,2] < 0 && result.table.bayes[i,4]>0){
    result.table.bayes$comment[i] <- "not sig. at 5%"
  } else {
      result.table.bayes$comment[i] <- ""
    }
}

if (SAVEPLOTS){
  write.csv(result.table.bayes, file = paste0(PLOTPATH, FILENAME, "results_table_bayes.csv"))
  } else {
    print(result.table.bayes)
  }
```



### Maximum Likelihood Estimation 


### Regression coefficient estimates and 95% Confidence Intervals (CI)
with their interpretation and data support (computed with structural MCMC).

Second table `spportdag` is the percentage of the individual arcs supported by the 
MCMC sample. 
```{r}
# Look-up table for interpretation of results depending on variable distribution
interpretLUT <- lapply(fabn.boot.th.mle$abnDag$data.dists, function(x){
  if(x=="binomial"){
    "odds ratio"
  } else if(x=="gaussian"){
    "correlation"
  } else if(x=="poission"){
    "rate ratio"}
})

# Extract each edge (cornames) and it's interpretation
cornames <- c()
interpret <- c()
for (i in 1:length(fabn.boot.th.mle$coef)){
  for (j in colnames(fabn.boot.th.mle$coef[[i]])){
    interpret <- c(interpret, interpretLUT[which(names(unlist(interpretLUT)) == names(fabn.boot.th.mle$coef[i]))][[1]])
    if(str_detect(j, "intercept")){
      cornames <- c(cornames, j)
    } else {
        cornames <- c(cornames, paste0(names(fabn.boot.th.mle$coef[i]), "|", j))
      }
  }
}

# each edge's support aka. arc-strength
support <- c()
supportdag <- dag.mcmc.boot
supportdag[supportdag<=THRESHOLD] <- 0
for (i in 1:length(cornames)){
  edgename <- str_split(cornames[[i]], pattern = "\\|", simplify = T)
  fromname <- edgename[2]
  toname <- edgename[1]
  fromidx <- which(colnames(supportdag) == fromname)
  toidx <- which(colnames(supportdag) == toname)
  support <- c(support, round(supportdag[toidx, fromidx], 2))
}

# put all together
result.table <- data.frame(cornames = cornames,
                           coefficient = unlist(fabn.boot.th.mle$coef, use.names = F),
                           SE = unlist(fabn.boot.th.mle$Stderror, use.names = F),
                           interpretation= interpret)

# remove edges with "intercept"
result.table <- result.table[which(!str_detect(result.table$cornames, "intercept")),]
# add support values
result.table <- cbind(result.table, support)

for (i in 1:nrow(result.table)){
  if(str_detect(result.table$interpretation[i], "ratio")){
    # Because of logit() scale
    result.table$coefficient[i] <- exp(result.table$coefficient[i])
  }
  
  # # exponentiate IAsize log
  # if(str_detect(result.table$cornames[i], "log")){
  #   result.table$coefficient[i] <- exp(result.table$coefficient[i])
  # }
  
  if(result.table$coefficient[i]<1 && result.table$interpretation[i] != "correlation"){
    result.table$association[i] <- "negative"
  } else if (result.table$coefficient[i]>1 && result.table$interpretation[i] != "correlation"){
    result.table$association[i] <- "positive"
  } else if (result.table$coefficient[i]>0 && result.table$interpretation[i] == "correlation"){
    result.table$association[i] <- "positive"
  } else if (result.table$coefficient[i]<0 && result.table$interpretation[i] == "correlation"){
    result.table$association[i] <- "negative"}
  
  # extract cornames for parent and children
  childrenParent <- str_split(result.table$cornames[i], "\\|")
  result.table$parent[i] <- childrenParent[[1]][2]
  result.table$children[i] <- childrenParent[[1]][[1]]
  
  # add levels
  levsparents <- levels(abndata[[result.table$parent[i]]])
  if(!is.null(levsparents)){
    result.table$parentLevels[i] <- paste(levsparents, collapse = '; ')
  } else if(is.null(levsparents) && (result.table$parent[i] == "IAsize_log" | result.table$parent[i] == "AgeDiag")){
    result.table$parentLevels[i] <- paste("continuous scale")
  }
  levschild <- levels(abndata[[result.table$children[i]]])
  if(!is.null(levschild)){
    result.table$childrenLevels[i] <- paste(levschild, collapse = '; ')
  } else if(is.null(levschild) && (result.table$children[i] == "IAsize_log" | result.table$children[i] == "AgeDiag")){
    result.table$childrenLevels[i] <- paste("continuous scale")
  }
}


# round and convert to integer
result.table.mle <- result.table %>%
  mutate(coefficient = as.numeric(round(coefficient, 2)),
         SE = as.numeric(round(SE, 2)))

if (SAVEPLOTS){
  write.csv(result.table.mle, file = paste0(PLOTPATH, FILENAME, "results_table_mle.csv"))
  write.csv(supportdag, file = paste0(PLOTPATH, FILENAME, "individual_arc_support.csv"))
  } else {
    print(result.table.mle)
  }
```
the odds and rate ratios: If smaller than one, a ratio has a negative effect. 
Inversely, if larger than one, the effect is positive.


#### interpretations:
```{r}
str(abndata)
abndata[1,]
```

Start with support == 1.
```{r}
result.table.mle[which(result.table.mle$support == 1),]
```



Arcs with less support
```{r}
result.table.mle[which(result.table.mle$support != 1),]
```


### Arc-histogram
```{r}
# Arc histogram
arcsdist <- apply(list.mc.out.burn.thin.dag, 3, sum)
plt <- barplot(table(arcsdist),col = "grey",xlab = "Number of arcs in the DAG", ylab = "Number of DAGs")

if (SAVEPLOTS){
  PLOTNAME <- "arc_histogram"
  dev.print(svg, filename = paste0(PLOTPATH, FILENAME, PLOTNAME, ".svg"), width = PLOTWIDTH, height = PLOTHEIGHT)
  plt
  dev.off()
} else {
  plt
}
```

### Most frequent DAGS in MCMC sample

```{r WARNING_takes_long_time, cache=TRUE}
u.list.dag <- unique.array(x = list.mc.out.burn.thin.dag,MARGIN = 3) # remove duplicate elements/rows

num_100 <- apply(X = u.list.dag, MARGIN = 3, FUN = function(x){
  sum(apply(X = list.mc.out.burn.thin.dag,MARGIN = 3,FUN = function(y){
    if(identical(x,y)){1}else{0}
  }))
})

max(which((cumsum(sort(num_100,decreasing = FALSE)))/1000<0.80,arr.ind = TRUE))
```


```{r, cache=TRUE}
freqDAGstoplot <- 100

##plot
scores.dags <- vector(length = freqDAGstoplot)
num.arcs <- vector(length = freqDAGstoplot)
shd <- vector(length = freqDAGstoplot)
for(i in 1:freqDAGstoplot){

  dag <- u.list.dag[,,order(num_100,decreasing = TRUE)[i]]
  colnames(dag) <- rownames(dag) <- names(dist)
  fabn <- fitAbn(dag = dag,
                 data.df = abndata,
                 data.dists = dist,
                 method = METHOD)
  scores.dags[i] <- -fabn$bic
  num.arcs[i] <- sum(dag)
  shd[i] <- compareDag(ref = u.list.dag[,,order(num_100,decreasing = TRUE)[1]],u.list.dag[,,order(num_100,decreasing = TRUE)[i]])$`Hamming-distance`
}

if (SAVEPLOTS){
  PLOTNAME <- "mcmc_diversity"
  svg(filename = paste0(PLOTPATH, FILENAME, PLOTNAME, ".svg"), width = PLOTWIDTH, height = PLOTHEIGHT)
  
  par(mar=c(5,4,4,4))
  plot(1:freqDAGstoplot, sort(num_100,decreasing = TRUE)[1:freqDAGstoplot], type = 'n',ylab = "",xlab = "Number of arcs",xaxt="n",yaxt="n", ylim = c(0,20))
  axis(2,at = c(0, 5,10,15, 20),labels = c("0.0%","0.5%","1%", "1.5%", "2%"),col.axis = "#4393C3")
  mtext("Occurence of DAGs", side=2, line=2, col="#4393C3")
  rect(1:freqDAGstoplot - .4, 0, 1:freqDAGstoplot + .4, sort(num_100,decreasing = TRUE)[1:freqDAGstoplot], col = '#4393C3')
  par(new = TRUE)
  plot(x = 1:freqDAGstoplot,y = scores.dags,col="red", type = 'b', lwd=2, axes = FALSE, xlab = "",ylab="")
  axis(4, col.axis = 'red')
  mtext("DAGs scores", side=4, line=2, col="red")
  axis(1, col.axis = 'black',at = 1:freqDAGstoplot,labels = num.arcs)
  axis(3, col.axis = 'orange',at = 1:freqDAGstoplot,labels = shd)
  mtext("Structural Hamming distances", side=3, line=2, col="orange")

  dev.off()
} else {
  
  par(mar=c(5,4,4,4))
  plot(1:freqDAGstoplot, sort(num_100,decreasing = TRUE)[1:freqDAGstoplot], type = 'n',ylab = "",xlab = "Number of arcs",xaxt="n",yaxt="n", ylim = c(0,20))
  axis(2,at = c(0, 5,10,15, 20),labels = c("0.0%","0.5%","1%", "1.5%", "2%"),col.axis = "#4393C3")
  mtext("Occurence of DAGs", side=2, line=2, col="#4393C3")
  rect(1:freqDAGstoplot - .4, 0, 1:freqDAGstoplot + .4, sort(num_100,decreasing = TRUE)[1:freqDAGstoplot], col = '#4393C3')
  par(new = TRUE)
  plot(x = 1:freqDAGstoplot,y = scores.dags,col="red", type = 'b', lwd=2, axes = FALSE, xlab = "",ylab="")
  axis(4, col.axis = 'red')
  mtext("DAGs scores", side=4, line=2, col="red")
  axis(1, col.axis = 'black',at = 1:freqDAGstoplot,labels = num.arcs)
  axis(3, col.axis = 'orange',at = 1:freqDAGstoplot,labels = shd)
  mtext("Structural Hamming distances", side=3, line=2, col="orange")

}
```

## Save Analysis Results for reporting

```{r}
obj <- ls() # save current workspace

if (SAVEPLOTS){
  filename_results <- "_analysis_results"
  save(list= obj, file = paste0(PLOTPATH, FILENAME, filename_results, ".RData"))
}
```
